{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fac0856",
   "metadata": {},
   "source": [
    "# Complete Neural Network Project: MNIST and MB Dataset Classification\n",
    "\n",
    "**Final Project - Introduction to Machine Learning**\n",
    "\n",
    "This notebook contains a complete, self-contained implementation of a feed-forward neural network for two classification tasks:\n",
    "1. **MNIST Digit Classification** - Multi-class classification of handwritten digits\n",
    "2. **MB Dataset Classification** - Binary classification for medical data (Control vs Fibrosis)\n",
    "\n",
    "## Features\n",
    "- Custom neural network implementation with ReLU activation and softmax output\n",
    "- Mini-batch gradient descent with cross-entropy loss\n",
    "- Hyperparameter optimization using smart phased sampling\n",
    "- Comprehensive evaluation and visualization of results\n",
    "- Self-contained: No external imports from local files required\n",
    "\n",
    "All code is included in this notebook. Simply run all cells to execute the complete project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537428a5",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e888586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! All required libraries imported.\n",
      "Current working directory: /Users/naamaavni/workspace/naama/intro_to_ml/final_project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Setup complete! All required libraries imported.\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bf78e",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "Complete implementation of a feed-forward neural network with:\n",
    "- ReLU activation for hidden layers\n",
    "- Softmax activation for output layer\n",
    "- Cross-entropy loss function\n",
    "- Mini-batch gradient descent\n",
    "- Xavier weight initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f2f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, epochs=50, batch_size=32, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.training_loss = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(2. / self.layer_sizes[i])\n",
    "            b = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def one_hot_encode(self, y, num_classes):\n",
    "        return np.eye(num_classes)[y.astype(int)]\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            a = self.relu(z)\n",
    "            activations.append(a)\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        a = self.softmax(z)\n",
    "        activations.append(a)\n",
    "        return activations, zs\n",
    "\n",
    "    def backward_propagation(self, X, y, activations, zs):\n",
    "        grads_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grads_b = [np.zeros_like(b) for b in self.biases]\n",
    "        m = X.shape[0]\n",
    "        num_layers = len(self.weights)\n",
    "        delta = activations[-1] - y\n",
    "        grads_w[-1] = np.dot(activations[-2].T, delta) / m\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "        for l in range(num_layers - 2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[l+1].T) * self.relu_derivative(zs[l])\n",
    "            grads_w[l] = np.dot(activations[l].T, delta) / m\n",
    "            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_w, grads_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "    # def fit(self, X, y, verbose=True):\n",
    "    #     start_time = time.time()\n",
    "    #     num_classes = np.unique(y).size\n",
    "    #     y_encoded = self.one_hot_encode(y, num_classes)\n",
    "    #     n = X.shape[0]\n",
    "    #     self.training_loss = []\n",
    "    #     for epoch in range(self.epochs):\n",
    "    #         indices = np.arange(n)\n",
    "    #         np.random.shuffle(indices)\n",
    "    #         X_shuffled = X[indices]\n",
    "    #         y_shuffled = y_encoded[indices]\n",
    "    #         for i in range(0, n, self.batch_size):\n",
    "    #             X_batch = X_shuffled[i:i+self.batch_size]\n",
    "    #             y_batch = y_shuffled[i:i+self.batch_size]\n",
    "    #             activations, zs = self.forward_propagation(X_batch)\n",
    "    #             grads_w, grads_b = self.backward_propagation(X_batch, y_batch, activations, zs)\n",
    "    #             self.update_parameters(grads_w, grads_b)\n",
    "    #         activations, _ = self.forward_propagation(X)\n",
    "    #         loss = -np.mean(np.sum(y_encoded * np.log(activations[-1] + 1e-8), axis=1))\n",
    "    #         self.training_loss.append(loss)\n",
    "    #         if verbose and (epoch % max(1, self.epochs // 10) == 0 or epoch == self.epochs - 1):\n",
    "    #             print(f'Epoch {epoch+1}/{self.epochs} - Loss: {loss:.4f}')\n",
    "    #     end_time = time.time()\n",
    "    #     return end_time - start_time\n",
    "\n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \n",
    "        Args:\n",
    "            X: Training features (n_features, n_samples)\n",
    "            y: Training labels (n_samples,)\n",
    "            verbose: Whether to print training progress\n",
    "        \"\"\"\n",
    "        # Ensure X is in correct format (features x samples)\n",
    "        if X.shape[0] != self.layer_sizes[0]:\n",
    "            X = X.T\n",
    "        \n",
    "        # One-hot encode the labels\n",
    "        num_classes = self.layer_sizes[-1]\n",
    "        y_one_hot = self.one_hot_encode(y, num_classes)\n",
    "        \n",
    "        # Training history\n",
    "        self.training_loss = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Training neural network with {self.epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Mini-batch training\n",
    "            indices = np.random.permutation(X.shape[1])\n",
    "            total_loss = 0\n",
    "            \n",
    "            for i in range(0, X.shape[1], self.batch_size):\n",
    "                batch_indices = indices[i:i+self.batch_size]\n",
    "                X_batch = X[:, batch_indices]\n",
    "                y_batch = y_one_hot[:, batch_indices]\n",
    "                \n",
    "                # Forward propagation\n",
    "                activations, z_values = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute loss (cross-entropy)\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(activations[-1] + 1e-15), axis=0))\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward propagation\n",
    "                weight_gradients, bias_gradients = self.backward_propagation(X_batch, y_batch, activations, z_values)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(weight_gradients, bias_gradients)\n",
    "            \n",
    "            avg_loss = total_loss / (X.shape[1] // self.batch_size + 1)\n",
    "            self.training_loss.append(avg_loss)\n",
    "            \n",
    "            if verbose and epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "            print(f\"Final loss: {self.training_loss[-1]:.4f}\")\n",
    "        \n",
    "        return training_time\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def plot_training_loss(self, save_path=None):\n",
    "        plt.plot(self.training_loss, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Neural Network class defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856735ca",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "These functions load and preprocess the MNIST and MB datasets. If the CSV files are not found, they fall back to sklearn datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6c3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"Load and preprocess MNIST dataset from experiments/mnist/inputs directory or fallback to sklearn digits.\"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv('experiments/mnist/inputs/MNIST-train.csv')\n",
    "        test_data = pd.read_csv('experiments/mnist/inputs/MNIST-test.csv')\n",
    "        \n",
    "        # Separate features and labels using 'y' column\n",
    "        X_train_full = train_data.drop('y', axis=1).values  # All columns except 'y'\n",
    "        y_train_full = train_data['y'].values               # 'y' column (labels)\n",
    "        \n",
    "        X_test = test_data.drop('y', axis=1).values    # All columns except 'y'\n",
    "        y_test = test_data['y'].values                 # 'y' column (labels)\n",
    "        \n",
    "        # Split training data into train and validation sets (90/10 split)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_full, y_train_full, test_size=0.1, random_state=42, stratify=y_train_full\n",
    "        )\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        X_train = X_train / 255.0\n",
    "        X_val = X_val / 255.0\n",
    "        X_test = X_test / 255.0\n",
    "        \n",
    "        print(f\"Loaded MNIST CSV: {X_train.shape[0]} train, {X_val.shape[0]} val, {X_test.shape[0]} test\")\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to sklearn digits: {e}\")\n",
    "        digits = load_digits()\n",
    "        X, y = digits.data, digits.target\n",
    "        \n",
    "        # Split into train, validation, and test sets (60/20/20 split)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 of remaining 80% = 20% of total\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded sklearn digits: {X_train.shape[0]} train, {X_val.shape[0]} val, {X_test.shape[0]} test\")\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def load_mb_data():\n",
    "    \"\"\"Load and preprocess MB dataset from experiments/mb/inputs directory or fallback to sklearn breast cancer.\"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv('experiments/mb/inputs/MB_data_train.csv', index_col=0)\n",
    "        y_train = []\n",
    "        for patient_id in train_data.index:\n",
    "            if patient_id.startswith('Pt_Fibro_'):\n",
    "                y_train.append(1)\n",
    "            elif patient_id.startswith('Pt_Ctrl_'):\n",
    "                y_train.append(0)\n",
    "            else:\n",
    "                y_train.append(0)\n",
    "        y_train = np.array(y_train)\n",
    "        X_train = train_data.values\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        print(f\"Loaded MB CSV: {X_train_split.shape[0]} train, {X_val.shape[0]} val\")\n",
    "        return X_train_split, X_val, y_train_split, y_val\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to sklearn breast cancer: {e}\")\n",
    "        cancer = load_breast_cancer()\n",
    "        X, y = cancer.data, cancer.target\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        print(f\"Loaded sklearn breast cancer: {X_train.shape[0]} train, {X_val.shape[0]} val\")\n",
    "        return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854f78",
   "metadata": {},
   "source": [
    "## Experiment and Hyperparameter Optimization Functions\n",
    "\n",
    "These functions run the experiments and optimize hyperparameters for both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eafe4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(X_train, y_train, X_val, y_val, max_trials=10, is_mb=False):\n",
    "    print(\"Optimizing hyperparameters for\", \"MB\" if is_mb else \"MNIST\")\n",
    "    input_size = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    if is_mb:\n",
    "        architectures = [\n",
    "            [input_size, 64, 32, num_classes],\n",
    "            [input_size, 128, 64, num_classes],\n",
    "            [input_size, 256, 128, 64, num_classes],\n",
    "            [input_size, 128, 128, num_classes],\n",
    "            [input_size, 64, 64, 64, num_classes],\n",
    "            [input_size, 512, 256, 128, 64, num_classes],\n",
    "            [input_size, 32, 32, 32, 32, num_classes],\n",
    "            [input_size, 256, 256, num_classes],\n",
    "        ]\n",
    "        learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "        batch_sizes = [4, 8, 16, 32, 64]\n",
    "    else:\n",
    "        architectures = [\n",
    "            [input_size, 64, 32, num_classes],\n",
    "            [input_size, 128, 64, num_classes],\n",
    "            [input_size, 256, 128, 64, num_classes],\n",
    "            [input_size, 128, 128, num_classes],\n",
    "            [input_size, 64, 64, 64, num_classes],\n",
    "            [input_size, 512, 256, 128, 64, num_classes],\n",
    "            [input_size, 32, 32, 32, 32, num_classes],\n",
    "            [input_size, 256, 256, num_classes],\n",
    "        ]\n",
    "        learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "        batch_sizes = [8, 16, 32, 64, 128]\n",
    "    epochs_list = [20, 30, 50, 100, 150]\n",
    "    results = []\n",
    "    best_accuracy = 0\n",
    "    best_config = None\n",
    "    for trial in range(max_trials):\n",
    "        arch = random.choice(architectures)\n",
    "        lr = random.choice(learning_rates)\n",
    "        epochs = random.choice(epochs_list)\n",
    "        batch_size = random.choice(batch_sizes)\n",
    "        nn = NeuralNetwork(\n",
    "            layer_sizes=arch,\n",
    "            learning_rate=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"Training trial {trial+1}/{max_trials}: neural network with architecture: {arch}, learning rate: {lr}, epochs: {epochs}, batch size: {batch_size}\")\n",
    "        training_time = nn.fit(X_train, y_train, verbose=False)\n",
    "        train_acc = nn.score(X_train, y_train)\n",
    "        val_acc = nn.score(X_val, y_val)\n",
    "        result = {\n",
    "            'trial': trial+1,\n",
    "            'architecture': arch,\n",
    "            'learning_rate': lr,\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'training_time': training_time,\n",
    "            'final_loss': nn.training_loss[-1] if nn.training_loss else None,\n",
    "            'total_params': sum(w.size + b.size for w, b in zip(nn.weights, nn.biases)),\n",
    "        }\n",
    "        results.append(result)\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_config = result.copy()\n",
    "        print(f\"Trial {trial+1}/{max_trials}: Validation Accuracy = {val_acc:.4f}\")\n",
    "    return best_config, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2714eab",
   "metadata": {},
   "source": [
    "## MNIST Experiment\n",
    "\n",
    "Load the data, optimize hyperparameters, train the final model, and visualize results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6998ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST CSV: 54000 train, 6000 val, 10000 test\n",
      "Optimizing hyperparameters for MNIST\n",
      "Training trial 1/10: neural network with architecture: [784, 512, 256, 128, 64, 10], learning rate: 0.5, epochs: 150, batch size: 16\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 21351 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m X_train_mnist, X_val_mnist, X_test_mnist, y_train_mnist, y_val_mnist, y_test_mnist = load_mnist_data()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_config_mnist, results_mnist = \u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_mnist\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest MNIST config:\u001b[39m\u001b[33m\"\u001b[39m, best_config_mnist)\n\u001b[32m      6\u001b[39m final_nn_mnist = NeuralNetwork(\n\u001b[32m      7\u001b[39m     layer_sizes=best_config_mnist[\u001b[33m'\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m     learning_rate=best_config_mnist[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m     epochs=best_config_mnist[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     batch_size=best_config_mnist[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36moptimize_hyperparameters\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, max_trials, is_mb)\u001b[39m\n\u001b[32m     40\u001b[39m nn = NeuralNetwork(\n\u001b[32m     41\u001b[39m     layer_sizes=arch,\n\u001b[32m     42\u001b[39m     learning_rate=lr,\n\u001b[32m     43\u001b[39m     epochs=epochs,\n\u001b[32m     44\u001b[39m     batch_size=batch_size\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: neural network with architecture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m training_time = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m train_acc = nn.score(X_train, y_train)\n\u001b[32m     49\u001b[39m val_acc = nn.score(X_val, y_val)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mNeuralNetwork.fit\u001b[39m\u001b[34m(self, X, y, verbose)\u001b[39m\n\u001b[32m    123\u001b[39m batch_indices = indices[i:i+\u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    124\u001b[39m X_batch = X[:, batch_indices]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m y_batch = \u001b[43my_one_hot\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[32m    128\u001b[39m activations, z_values = \u001b[38;5;28mself\u001b[39m.forward_propagation(X_batch)\n",
      "\u001b[31mIndexError\u001b[39m: index 21351 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "X_train_mnist, X_val_mnist, X_test_mnist, y_train_mnist, y_val_mnist, y_test_mnist = load_mnist_data()\n",
    "best_config_mnist, results_mnist = optimize_hyperparameters(\n",
    "    X_train_mnist, y_train_mnist, X_val_mnist, y_val_mnist\n",
    ")\n",
    "print(\"Best MNIST config:\", best_config_mnist)\n",
    "final_nn_mnist = NeuralNetwork(\n",
    "    layer_sizes=best_config_mnist['architecture'],\n",
    "    learning_rate=best_config_mnist['learning_rate'],\n",
    "    epochs=best_config_mnist['epochs'],\n",
    "    batch_size=best_config_mnist['batch_size']\n",
    ")\n",
    "final_nn_mnist.fit(X_train_mnist, y_train_mnist, verbose=True)\n",
    "final_val_accuracy_mnist = final_nn_mnist.score(X_val_mnist, y_val_mnist)\n",
    "final_test_accuracy_mnist = final_nn_mnist.score(X_test_mnist, y_test_mnist)\n",
    "print(f\"Final MNIST Validation Accuracy: {final_val_accuracy_mnist:.4f}\")\n",
    "print(f\"Final MNIST Test Accuracy: {final_test_accuracy_mnist:.4f}\")\n",
    "final_nn_mnist.plot_training_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99b5f3",
   "metadata": {},
   "source": [
    "## MB Experiment\n",
    "\n",
    "Load the data, optimize hyperparameters, train the final model, and visualize results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77735d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mb, X_val_mb, y_train_mb, y_val_mb = load_mb_data()\n",
    "best_config_mb, results_mb = optimize_hyperparameters(\n",
    "    X_train_mb, y_train_mb, X_val_mb, y_val_mb, max_trials=10, is_mb=True\n",
    ")\n",
    "print(\"\n",
    "Best MB config:\", best_config_mb)\n",
    "final_nn_mb = NeuralNetwork(\n",
    "    layer_sizes=best_config_mb['architecture'],\n",
    "    learning_rate=best_config_mb['learning_rate'],\n",
    "    epochs=best_config_mb['epochs'],\n",
    "    batch_size=best_config_mb['batch_size']\n",
    ")\n",
    "final_nn_mb.fit(X_train_mb, y_train_mb, verbose=True)\n",
    "final_val_accuracy_mb = final_nn_mb.score(X_val_mb, y_val_mb)\n",
    "print(f\"\n",
    "Final MB Validation Accuracy: {final_val_accuracy_mb:.4f}\")\n",
    "final_nn_mb.plot_training_loss()\n",
    "print(\"\n",
    "Classification Report:\")\n",
    "print(classification_report(y_val_mb, final_nn_mb.predict(X_val_mb), target_names=['Control', 'Fibrosis']))\n",
    "print(\"\n",
    "Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_mb, final_nn_mb.predict(X_val_mb)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
